{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "imposed-corpus",
        "zJR9huH3k6ks",
        "coordinate-better",
        "XL5Z74cVvf2d"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a_EbVVZDOcg"
      },
      "source": [
        ""
      ],
      "id": "_a_EbVVZDOcg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phantom-agenda"
      },
      "source": [
        "import tensorflow as tf\n",
        "import gin.tf\n",
        "import numpy as np\n",
        "import functools\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional"
      ],
      "id": "phantom-agenda",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqRhcUSIt9W_"
      },
      "source": [
        "gin.enter_interactive_mode()"
      ],
      "id": "iqRhcUSIt9W_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imposed-corpus"
      },
      "source": [
        "### SelfAttentionLayer"
      ],
      "id": "imposed-corpus"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "superior-uruguay"
      },
      "source": [
        "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, heads_count: int, attention_head_dimension: int, dropout_rate: float = 0.0):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        self.attention_layer = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=heads_count,\n",
        "            key_dim=attention_head_dimension,\n",
        "            dropout=dropout_rate,\n",
        "        )\n",
        "    \n",
        "    def call(self, inputs, mask=None, training=None):\n",
        "        return self.attention_layer(query=inputs, value=inputs, attention_mask=mask, training=training)"
      ],
      "id": "superior-uruguay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sustained-crawford",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a713e790-56ac-4e8d-8338-c57f9c97b1d9"
      },
      "source": [
        "layer = SelfAttentionLayer(heads_count=4, attention_head_dimension=256, dropout_rate=0.5)\n",
        "embeddings = tf.random.uniform(shape=(64, 10, 256))\n",
        "layer(embeddings).shape"
      ],
      "id": "sustained-crawford",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 10, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJR9huH3k6ks"
      },
      "source": [
        "### PointwiseFeedforwardLayer"
      ],
      "id": "zJR9huH3k6ks"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iFFuVfrk66i"
      },
      "source": [
        "class PointwiseFeedforwardLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, hidden_layer_dimension: int):\n",
        "        super(PointwiseFeedforwardLayer, self).__init__()\n",
        "        self.hidden_layer_dimension = hidden_layer_dimension\n",
        "        self.dense_layer1 = None\n",
        "        self.dense_layer2 = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.dense_layer1 = tf.keras.layers.Dense(\n",
        "            self.hidden_layer_dimension, activation=\"relu\"\n",
        "        )\n",
        "        self.dense_layer2 = tf.keras.layers.Dense(units=input_shape[-1])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        outputs = self.dense_layer1(inputs, training=training)\n",
        "        return self.dense_layer2(outputs, training=training)"
      ],
      "id": "8iFFuVfrk66i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMDMaQKjm4Wo",
        "outputId": "17996669-3ab3-4b66-cf94-c615eef09156"
      },
      "source": [
        "layer = PointwiseFeedforwardLayer(hidden_layer_dimension=512)\n",
        "layer(tf.random.uniform(shape=(32, 10, 256))).shape"
      ],
      "id": "vMDMaQKjm4Wo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 10, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coordinate-better"
      },
      "source": [
        "### TransformerEncoderLayer"
      ],
      "id": "coordinate-better"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "challenging-preference"
      },
      "source": [
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_heads_count: int,\n",
        "        attention_head_dimension: int,\n",
        "        pointwise_hidden_layer_dimension: int,\n",
        "        dropout_rate: float = 0.0,\n",
        "    ):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.attention_layer = SelfAttentionLayer(\n",
        "            heads_count=attention_heads_count,\n",
        "            attention_head_dimension=attention_head_dimension,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        self.pointwise_layer = PointwiseFeedforwardLayer(\n",
        "            hidden_layer_dimension=pointwise_hidden_layer_dimension,\n",
        "        )\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout_layer1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout_layer2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        attention_outputs = self.attention_layer(inputs, training=training)\n",
        "        attention_outputs = self.dropout_layer1(attention_outputs, training=training)\n",
        "        attention_outputs = self.layer_norm1(inputs + attention_outputs)\n",
        "        pointwise_outputs = self.pointwise_layer(attention_outputs)\n",
        "        pointwise_outputs = self.dropout_layer2(pointwise_outputs, training=training)\n",
        "        return self.layer_norm2(attention_outputs + pointwise_outputs)"
      ],
      "id": "challenging-preference",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2GLb9Dv2OLj",
        "outputId": "6dafea93-f5f8-4c44-bdba-0b487447f0a1"
      },
      "source": [
        "layer = TransformerEncoderLayer(\n",
        "    attention_heads_count=4,\n",
        "    attention_head_dimension=512,\n",
        "    pointwise_hidden_layer_dimension=512,\n",
        "    dropout_rate=0.5,\n",
        ")\n",
        "layer(tf.random.uniform(shape=(32, 10, 256))).shape, layer.count_params()"
      ],
      "id": "V2GLb9Dv2OLj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 10, 256]), 2367488)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL5Z74cVvf2d"
      },
      "source": [
        "### StackedTransformerEncoderLayers"
      ],
      "id": "XL5Z74cVvf2d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhumttmqvgCN"
      },
      "source": [
        "class StackedTransformerEncodersLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            layers_count: int = gin.REQUIRED,\n",
        "            attention_heads_count: int = gin.REQUIRED,\n",
        "            attention_head_dimension: int = gin.REQUIRED,\n",
        "            pointwise_hidden_layer_dimension: int = gin.REQUIRED,\n",
        "            dropout_rate: float = gin.REQUIRED,\n",
        "    ):\n",
        "        super(StackedTransformerEncodersLayer, self).__init__()\n",
        "        encoder_layer_initializer = functools.partial(\n",
        "            TransformerEncoderLayer,\n",
        "            attention_heads_count=attention_heads_count,\n",
        "            attention_head_dimension=attention_head_dimension,\n",
        "            pointwise_hidden_layer_dimension=pointwise_hidden_layer_dimension,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        self.sublayers = [encoder_layer_initializer() for _ in range(layers_count)]\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        outputs = inputs\n",
        "        for sublayer in self.sublayers:\n",
        "            outputs = sublayer(outputs, training=training)\n",
        "        return outputs"
      ],
      "id": "yhumttmqvgCN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "academic-thanksgiving",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d4e0fe-7f2c-4e38-c74e-51b9bee38ad4"
      },
      "source": [
        "layer = StackedTransformerEncodersLayer(\n",
        "    layers_count=12,\n",
        "    attention_heads_count=8,\n",
        "    attention_head_dimension=512,\n",
        "    pointwise_hidden_layer_dimension=2048,\n",
        "    dropout_rate=0.5,\n",
        ")\n",
        "layer(tf.random.uniform(shape=(32, 10, 512))).shape, layer.count_params()"
      ],
      "id": "academic-thanksgiving",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 10, 512]), 126038016)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvlsut3FESrP"
      },
      "source": [
        "### Embeddings layers"
      ],
      "id": "zvlsut3FESrP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNKhpxFet7t6"
      },
      "source": [
        "class PositionEmbeddingsLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, max_inputs_length: int, use_fourier_series: bool, trainable: bool):\n",
        "        super(PositionEmbeddingsLayer, self).__init__()\n",
        "        self.max_inputs_length = max_inputs_length\n",
        "        self.use_fourier_series = use_fourier_series\n",
        "        self.trainable = trainable\n",
        "        self.position_embeddings = None\n",
        "\n",
        "    def _get_fourier_angles(self, embeddings_dimension):\n",
        "        input_positions = np.arange(self.max_inputs_length).reshape((-1, 1))\n",
        "        embedding_positions = np.arange(embeddings_dimension).reshape((1, -1))\n",
        "        relative_embeddings_positions = (2.0 * (embedding_positions // 2)) / embeddings_dimension\n",
        "        return input_positions / np.power(10000, relative_embeddings_positions)\n",
        "\n",
        "    def _get_fourier_positional_embeddings(self, embeddings_dimension):\n",
        "        angles = self._get_fourier_angles(embeddings_dimension)\n",
        "        positional_embeddings = np.zeros(angles.shape)\n",
        "        positional_embeddings[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        positional_embeddings[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        return positional_embeddings\n",
        "\n",
        "    def _get_initial_embeddings(self, embeddings_dimension):\n",
        "        if self.use_fourier_series:\n",
        "            return self._get_fourier_positional_embeddings(embeddings_dimension)\n",
        "        return tf.random.truncated_normal(shape=(self.max_inputs_length, embeddings_dimension))\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        initial_embeddings = self._get_initial_embeddings(\n",
        "            embeddings_dimension=input_shape[-1]\n",
        "        )\n",
        "        self.position_embeddings = tf.Variable(\n",
        "            initial_embeddings,\n",
        "            name='position_embeddings',\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        inputs_length = tf.shape(inputs)[-2]\n",
        "        chosen_embeddings = self.position_embeddings[:inputs_length, :]\n",
        "        return tf.broadcast_to(chosen_embeddings, inputs.shape)"
      ],
      "id": "GNKhpxFet7t6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCeqRPCGuGo3",
        "outputId": "e41ded90-9e43-4ab3-873b-974395182d31"
      },
      "source": [
        "layer = PositionEmbeddingsLayer(\n",
        "    max_inputs_length=12,\n",
        "    use_fourier_series=True,\n",
        "    trainable=True,\n",
        ")\n",
        "layer(tf.random.uniform(shape=(64, 10, 256))).shape"
      ],
      "id": "lCeqRPCGuGo3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 10, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}